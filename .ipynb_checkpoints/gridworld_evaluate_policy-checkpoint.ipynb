{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Iterative Policy Evaluation in a grid world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we will derive it manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from Figure 4.5 that the algorithm we're trying to apply is:\n",
    "    \n",
    "$$V(s) \\leftarrow \\sum_a \\pi (s,a) \\sum_{s'} P^a_{ss'} [ R^a_{ss'}+ \\gamma V(s)]$$\n",
    "    \n",
    "\n",
    "We start with the state value function, in matrix form, as all zeros:\n",
    "```\n",
    "[0,   0,   0,   0,   0]\n",
    "[0,   0,   0,   0,   0]\n",
    "[0,   0,   0,   0,   0]\n",
    "[0,   0,   0,   0,   0]\n",
    "[0,   0,   0,   0,   0]\n",
    "```\n",
    "The first step of the update process is to iterate over states, starting with (0, 0). The possible actions of North, South, East, West. Since actions are deterministic, there is only one $s'$ with a non-zero probability, so the summation over states simplifies to the one statement. For instance, if our environment was stochastic, we could go imagine our action being North from (0, 0), and ending up in (0,0), (0,1), or (1,0). We can express this as:\n",
    "\n",
    "$$\\sum_{s'} P^a_{ss'} * X(s')\\ , where\\ s = (0, 0), a = North $$\n",
    "$$P^{North}_{(0,0), (0,0)} * X(0,0) + P^{North}_{(0,0), (1,0)} * X(1,0) + P^{North}_{(0,0), (1,0)} * X(0,1) $$\n",
    "$$P^{North}_{(0,0), (0,0)} * X(0,0) + P^{North}_{(0,0), (1,0)} * X(1,0) + P^{North}_{(0,0), (1,0)} * X((0,1) $$\n",
    "$$1.0 * X(0.0) + 0.0 * X(1,0) + 0.0 * X(0,1) $$\n",
    "$$X(0.0)$$\n",
    "\n",
    "So intuitively, at (0, 0), we update V(0,0) to be the one-step rewards of taking each step, weight by 0.25 because our policy is random. Formally:\n",
    "\n",
    "$$V(0,0) \\leftarrow 0.25*-1+0.25*0+0.25*0+0.25*-1$$\n",
    "$$V(0,0) \\leftarrow -0.5$$\n",
    "\n",
    "We continue this process for all units. The value will be -0.5 for all corner states, and -0.25 for edge states, other than A and B, which are the only states with rewards greater than zero. The value will be 0 for all non-edge, non-corner states. For A, the rewards for each action are 10. For the action North, the new V would be equal to 10.  I'm going to abuse Q(s,a) notation here to be clear that this isn't the whole summation over actions, but just one action.\n",
    "\n",
    "$$Q((1,0), N) = P^{north}_{(1,0)(1,4)} * [R^{north}_{(1,0)(1,4)} + \\gamma V(0,0)]$$\n",
    "$$Q((1,0), N) = 1 * [10 + 0.9 * 0.0]$$\n",
    "$$Q((1,0), N) = 10$$\n",
    "\n",
    "The Q values for each other action are equal, since going any direction out of A gives reward 10. Therefore, the sum of state-action values, weighted by the policy (0.25), ends up being 10 total.\n",
    "By the same math, B is equal to 5. Now our state value looks like this:\n",
    "```\n",
    "[-.50,   10, -.25,   10, -.50]\n",
    "[-.25,    0,    0,    0, -.25]\n",
    "[-.25,    0,    0,    0, -.25]\n",
    "[-.25,    0,    0,    0, -.25]\n",
    "[-.50, -.25, -.25, -.25, -.50]\n",
    "```\n",
    "\n",
    "If we return for our next sweep over the states, we will see that things change for (0,0). The values of future states now have non-zero state value estimates.\n",
    "\n",
    "$$Q((0,0), North) = R^{North}_{(0,0)(0,0)} + \\gamma V(0,0)$$\n",
    "$$Q((0,0), North) = -1 + 0.9 * -0.5$$\n",
    "$$Q((0,0), North) = -1.45$$\n",
    "\n",
    "$$Q((0,0), South) = R^{South}_{(0,0)(0,1)} + \\gamma V(0,1)$$\n",
    "$$Q((0,0), South) = 0 + 0.9 * -.25$$\n",
    "$$Q((0,0), South) = -.225$$\n",
    "\n",
    "$$Q((0,0), East) = R^{East}_{(0,0)(1,0)} + \\gamma V(1,0)$$\n",
    "$$Q((0,0), East) = 0 + 0.9 * 10$$\n",
    "$$Q((0,0), East) = 9$$\n",
    "\n",
    "$$Q((0,0), West) = R^{West}_{(0,0)(0,10} + \\gamma V(0,0)$$\n",
    "$$Q((0,0), West) = -1 + 0.9 * -.5$$\n",
    "$$Q((0,0), West) = -1.45$$\n",
    "\n",
    "Weighting the values of each action by their probability under policy $\\pi$, we get:\n",
    "\n",
    "$$V(0,0) \\leftarrow 0.25*Q((0,0), North)+0.25*Q((0,0), South)+0.25*Q((0,0), East)+0.25*Q((0,0), West)$$\n",
    "$$V(0,0) \\leftarrow 0.25 * -1.45 + 0.25 * -0.225 + 0.25 * 9 + 0.25 * -1.45$$\n",
    "$$V(0,0) \\leftarrow 1.46875$$\n",
    "\n",
    "Haha! We're finally getting some interesting numbers. The value of A also changes too, because we now have a non-zero V(1,4).\n",
    "\n",
    "$$Q((1,0), East) = R^{East}_{(1,0)(1,4)} + \\gamma V(1,4)$$\n",
    "$$Q((1,0), East) = 10 + 0.9 * -0.25$$\n",
    "$$Q((1,0), East) = 9.775$$\n",
    "\n",
    "Now that we see how the manual process works, let's let the code finish it off for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def R(state, action, next_state):\n",
    "    r = state[0]\n",
    "    c = state[1]\n",
    "    if c == 1 and r == 0:\n",
    "        return 10\n",
    "    elif c == 3 and r == 0:\n",
    "        return 5\n",
    "    elif action == 'N':\n",
    "        if r == 0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    elif action == 'S':\n",
    "        if r == 4:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    elif action == 'E':\n",
    "        if c == 4:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    elif action == 'W':\n",
    "        if c == 0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "def P(state, action, next_state):\n",
    "    r = state[0]\n",
    "    c = state[1]\n",
    "    if c == 1 and r == 0:\n",
    "        s = (4,1)\n",
    "    elif c == 3 and r == 0:\n",
    "        s = (2,3)\n",
    "    elif action == 'N':\n",
    "        if state[0] == 0:\n",
    "            s = (r, c)\n",
    "        else:\n",
    "            s = (r - 1, c)\n",
    "    elif action == 'S':\n",
    "        if state[0] == 4:\n",
    "            s = (r, c)\n",
    "        else:\n",
    "            s = (r + 1, c)\n",
    "    elif action == 'E':\n",
    "        if state[1] == 4:\n",
    "            s = (r, c)\n",
    "        else:\n",
    "            s = (r, c + 1)\n",
    "    elif action == 'W':\n",
    "        if state[1] == 0:\n",
    "            s = (r, c)\n",
    "        else:\n",
    "            s = (r, c - 1)\n",
    "            \n",
    "    if next_state[0] == s[0] and next_state[1] == s[1]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def A(state):\n",
    "    return ['N', 'S', 'E', 'W']\n",
    "\n",
    "def next_states(state):\n",
    "    r = state[0]\n",
    "    c = state[1]\n",
    "    if c == 1 and r == 0:\n",
    "        return [(4,1)]\n",
    "    elif c == 3 and r == 0:\n",
    "        return [(2,3)]\n",
    "    return set([(r - 1 if r > 0 else r,c ),\n",
    "           (r + 1 if r < 4 else r, c),\n",
    "           (r, c - 1 if c > 0 else c),\n",
    "           (r, c + 1 if c < 4 else c)])\n",
    "\n",
    "def S():\n",
    "    states = []\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            states.append((i, j))\n",
    "    return states\n",
    "\n",
    "def Pi(state, action):\n",
    "    return 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "Now that we have our P, R, and Pi, lets iteratively solve the system of linear equations that is the Value of policy $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged with delta 0.00 at iter 323\n",
      "[[ 3.3  8.8  4.4  5.3  1.5]\n",
      " [ 1.5  3.   2.3  1.9  0.5]\n",
      " [ 0.1  0.7  0.7  0.4 -0.4]\n",
      " [-1.  -0.4 -0.4 -0.6 -1.2]\n",
      " [-1.9 -1.3 -1.2 -1.4 -2. ]]\n"
     ]
    }
   ],
   "source": [
    "V = np.zeros((5, 5))\n",
    "gamma = 0.9\n",
    "iters = 0\n",
    "converged = False\n",
    "while not converged: \n",
    "    delta = 0\n",
    "    newV = np.zeros((5,5))\n",
    "    for s in S():\n",
    "        v = V[s]\n",
    "        sum_over_actions = 0\n",
    "        for a in A(s): # this corresponds to the summation over actions\n",
    "            sum_over_next_states = 0\n",
    "            for next_state in next_states(s): # this corresponds to the summation over next states\n",
    "                p = P(s, a, next_state)\n",
    "                r = R(s, a, next_state)\n",
    "                x = gamma * V[next_state]\n",
    "                sum_over_next_states += p * (r + x)\n",
    "            sum_over_actions += Pi(s, a) * sum_over_next_states\n",
    "        newV[s] = sum_over_actions\n",
    "        delta = np.fmax(delta, abs(newV[s] - v))\n",
    "\n",
    "    if delta == 0.0:\n",
    "        print(\"Converged with delta {:.2f} at iter {}\".format(delta, iters))\n",
    "        print(np.round(V, 1))\n",
    "        converged = True\n",
    "\n",
    "    iters += 1    \n",
    "#     print(newV)\n",
    "    V = newV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well, there you have it. It matches the results of Sutto & Barto exactly (as it should!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, there's another version of this algorithm that doesn't store updates to V(s) in a seperate array, but rather replaces them as we sweep over the states. We can see that this version actually converges faster than the previous version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
